{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/liraop/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liraop/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import nltk \n",
    "from nltk import RegexpTokenizer as rpt\n",
    "from nltk.corpus import stopwords as sw\n",
    "from string import punctuation \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopwords = sw.words('portuguese')\n",
    "\n",
    "data_url=\"https://raw.githubusercontent.com/liraop/recinfo_lab2/master/data/results.csv\"\n",
    "data = pd.read_csv(data_url).replace(np.nan, '', regex=True)\n",
    "documents = data.text.count()\n",
    "items = []\n",
    "\n",
    "def parse(text):\n",
    "    words = []\n",
    "    word_pattern = rpt(r'\\w+')\n",
    "    year_pattern = rpt(r'\\d{4}')\n",
    "    \n",
    "    patterns = [word_pattern, year_pattern]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        tokens = []\n",
    "        for token in pattern.tokenize(text):\n",
    "            if token not in stopwords and len(token) > 3:\n",
    "                tokens.append(token)\n",
    "        words.extend(tokens)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Execute o algoritmo ilustrado na Fig. 5.8 do livro texto (pag. 157) para gerar um índice similar o mostrado na Fig. 5.4 (pag. 134). Guarde o índice em disco em formato csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(dataset):\n",
    "    document_index = 0\n",
    "    index = {\"doc_row\": []}\n",
    "    \n",
    "    for entry in dataset.text:\n",
    "        document_index = document_index + 1\n",
    "        index[\"doc_row\"].append(document_index)\n",
    "            \n",
    "        for ngram in parse(entry):\n",
    "                if ngram in index: #is ngram already on index?\n",
    "                    if document_index in index[ngram]: # is it in the same document?\n",
    "                        index[ngram][document_index] = index[ngram][document_index] + 1\n",
    "                    else: # nope\n",
    "                        index[ngram][document_index] = 1 \n",
    "                else: # no, sir\n",
    "                    index[ngram] = {document_index: 1}\n",
    "    \n",
    "    return index\n",
    "\n",
    "index = build_index(data)\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('index.csv', 'w') as f:\n",
    "    w = csv.DictWriter(f, index.keys())\n",
    "    w.writeheader()\n",
    "    w.writerow(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implemente as abordagens de processamento de consulta documento-por-vez e termo-por-vez (Fig. 5.16 e 5.18). \n",
    "+ Defina 5 consultas de um termo somente. \n",
    "+ Execute as 5 consultas em cada algoritmo retornando os top-10 documentos (parâmetro k do algoritmo).\n",
    "+ Dê evidências de que sua implementação está correta.\n",
    "+ Compare os tempos médios de execução e uso de memória de cada algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"juíza\",\"federal\",\"governo\",\"Brasil\",\"presidente\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: juíza  - Rank: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Term: federal  - Rank: [151, 7, 37, 206, 213, 1, 2, 3, 15, 42]\n",
      "Term: governo  - Rank: [173, 166, 84, 210, 25, 82, 150, 221, 115, 168]\n",
      "Term: Brasil  - Rank: [151, 166, 19, 26, 248, 172, 173, 238, 22, 201]\n",
      "Term: presidente  - Rank: [63, 166, 151, 19, 216, 205, 86, 25, 103, 174]\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "def document_at_a_time(query, index, k):\n",
    "    rank = {}\n",
    "    inverted_lists = []\n",
    "    #query as space separated string list\n",
    "    for ngram in query.split(): \n",
    "        if ngram in index:\n",
    "            inverted_lists.append(index[ngram])\n",
    "            \n",
    "    for document_id in index['doc_row']:\n",
    "        rank[document_id] = 0\n",
    "        for i_list in inverted_lists:\n",
    "            for il_doc_id, il_doc_wc in i_list.items():\n",
    "                if il_doc_id == document_id:\n",
    "                    rank[document_id] = rank[document_id] + il_doc_wc\n",
    "                     \n",
    "    return heapq.nlargest(k, rank, rank.get)\n",
    "\n",
    "for query in queries:\n",
    "    print(\"Term:\",query, \" - Rank:\", document_at_a_time(query, index, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Esta implementação pode ser mais lenta no geral pois não utiliza a tabela de acumuladores como na estratégia de termo por vez. Contudo, utiliza bem menos memória exatamente pelo mesmo fato, além de ser uma abordagem de mais simples implementação.\n",
    "#### O ponto fraco do algoritmo, como fica evidente na implementação, é o fato de ter de sempre percorrer todo o índice para fazer o scoring, logo, várias otimizações são necessárias numa aplicação real. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: juíza - Rank: [1, 2]\n",
      "Term: federal - Rank: [151, 7, 37, 206, 213, 1, 2, 3, 15, 42]\n",
      "Term: governo - Rank: [173, 166, 84, 210, 25, 82, 150, 221, 115, 168]\n",
      "Term: Brasil - Rank: [151, 166, 19, 26, 248, 172, 173, 238, 22, 201]\n",
      "Term: presidente - Rank: [63, 166, 151, 19, 216, 205, 86, 25, 103, 174]\n"
     ]
    }
   ],
   "source": [
    "def term_at_a_time(query, index, k):\n",
    "    rank = {}\n",
    "    inverted_lists = []\n",
    "    #query as space separated string list\n",
    "    for ngram in query.split(): \n",
    "        if ngram in index:\n",
    "            inverted_lists.append(index[ngram])\n",
    "    \n",
    "    for i_list in inverted_lists:\n",
    "        for il_doc_id, il_doc_wc in i_list.items():\n",
    "            if il_doc_id not in rank:\n",
    "                rank[il_doc_id] = il_doc_wc\n",
    "            else:\n",
    "                rank[il_doc_id] = rank[il_doc_id] + il_doc_wc\n",
    "    \n",
    "    return heapq.nlargest(k, rank, rank.get)\n",
    "\n",
    "for query in queries:\n",
    "    print(\"Term:\",query, \"- Rank:\", term_at_a_time(query, index, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nesta implementação tomei a liberdade de não implementar a tabela de acumuladores por questões de praticidade. A tabela, entretanto, oferece a vantagem de ter acesso direto às colunas, visto que é uma hashtable, melhorando o desempenho geral do algoritmo devido ao acesso ai disco otimizado.\n",
    "\n",
    "#### Vemos que há poucas diferenças nos ranks de ambas implementações, provando que as implementações estão majoritariamente corretas, apesar das modificações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implemente uma das versões de consulta conjuntiva (AND) (Fig. 5.20 ou 5.21).\n",
    "+ Defina 5 consultas conjuntivas (AND).\n",
    "+ Execute as 5 consultas em cada algoritmo retornando os top-10 documentos(parâmetro k do algoritmo).\n",
    "+ Dê evidências de que sua implementação está correta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_at_a_time_conj():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
